{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 8248603,
          "sourceType": "datasetVersion",
          "datasetId": 4894044
        }
      ],
      "dockerImageVersionId": 30733,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Archimedes AI - Build with Claude June 2024 Developer Contest"
      ],
      "metadata": {
        "id": "hVT5FLXXlSDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Authors**: Antek Hasiura (Princeton Univeristy) & Jędrzej Hasiura (Eindhoven University of Technology)\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "**Project Goal**: develop an AI research assistant (\"Archimedes\") that could accelerate the academic research process through a RAG-based scholarly work retrieval, leveraging recent discoveries on the efficiency of LLMs in markdown processing, chain-of-thought prompting, and reranking.\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "**Motivation**:\n",
        "* High volume of published papers presents challenges to researchers who find it difficult to stay up to date with relevant work in their field\n",
        "* Tools for literature review research are outdated and the space continues to not be disrupted by recent developments in LLMs\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "In this notebook, we showcase the backend RAG model we developed for Archimedes. At the end of our work, we discuss what we envision Archimedes could do if we continued to develop the project further. Our initial UI drafts can also be viewed in the provided repository.\n",
        "\n",
        "<br/>\n",
        "\n",
        "\n",
        "**References**:\n",
        "\n",
        "<u>Markdowns</u>\n",
        "\n",
        "[1] Revolutionizing Retrieval-Augmented Generation with Enhanced PDF Structure Recognition [https://arxiv.org/abs/2401.12599]\n",
        "\n",
        "[2] Nougat: Neural Optical Understanding for Academic Documents [https://arxiv.org/abs/2308.13418]\n",
        "\n",
        "[3] Marker [https://github.com/VikParuchuri/marker]\n",
        "<br/><br/>\n",
        "\n",
        "\n",
        "<u>Retrieval Augmented Generation</u>\n",
        "\n",
        "[4] Azure AI Search: Outperforming vector search with hybrid retrieval and ranking capabilities [https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/azure-ai-search-outperforming-vector-search-with-hybrid/ba-p/3929167]\n",
        "\n",
        "[5] Retrieval-Augmented Generation for Large Language Models: A Survey [https://arxiv.org/pdf/2312.10997]\n",
        "\n",
        "[6] Introducing Rerank 3: A New Foundation Model for Efficient Enterprise Search & Retrieval [https://cohere.com/blog/rerank-3]\n",
        "\n",
        "[7] Developing and Evaluating RAG Solution: Generate embeddings [https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/rag/rag-generating-embeddings]\n",
        "\n",
        "[8] Build with Claude: Embeddings [https://docs.anthropic.com/en/docs/build-with-claude/embeddings]\n",
        "<br/><br/>\n",
        "\n",
        "<u>Prompting</u>\n",
        "\n",
        "[9] Chain-of-Thought Prompting Elicits Reasoning in Large Language Models [https://arxiv.org/abs/2201.11903]\n",
        "\n",
        "[10] The Prompt Report: A Systematic Survey of Prompting Techniques [https://arxiv.org/pdf/2406.06608]\n",
        "\n",
        "[11] Anthropic Prompt Generator [https://console.anthropic.com]\n",
        "\n",
        "[12] Emergent Abilities of Large Language Models [https://arxiv.org/pdf/2206.07682]\n",
        "<br/><br/>\n",
        "\n",
        "<u>Benchmarks </u>\n",
        "\n",
        "[13] MTEB Embedding Benchmark - [https://huggingface.co/spaces/mteb/leaderboard, https://arxiv.org/abs/2210.07316]\n",
        "\n",
        "[14] Claude 3.5 Sonnet comparison with GPT-4o [https://www.anthropic.com/news/claude-3-5-sonnet]"
      ],
      "metadata": {
        "id": "LsFsWbQ7lSDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Initial Setup"
      ],
      "metadata": {
        "id": "84lxDMuulSDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain langchain-chroma langchain-openai sentence-transformers langchain-cohere langchain-anthropic fastembed langchain-community langchain-voyageai langchain-cohere"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-07-09T00:36:50.601782Z",
          "iopub.execute_input": "2024-07-09T00:36:50.602150Z",
          "iopub.status.idle": "2024-07-09T00:37:57.311973Z",
          "shell.execute_reply.started": "2024-07-09T00:36:50.602119Z",
          "shell.execute_reply": "2024-07-09T00:37:57.311006Z"
        },
        "trusted": true,
        "id": "vV2ZN34glSDh",
        "outputId": "be9a38b7-74cf-4686-a81d-9f2a4c1cce64",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 24.4.1 requires cubinlinker, which is not installed.\ncudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 24.4.1 requires ptxcompiler, which is not installed.\ncuml 24.4.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 24.4.1 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.9.0 requires keras-core, which is not installed.\nkeras-nlp 0.12.1 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\naiobotocore 2.13.0 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\naiobotocore 2.13.0 requires botocore<1.34.107,>=1.34.70, but you have botocore 1.34.141 which is incompatible.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\ncudf 24.4.1 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.5.0 which is incompatible.\ndistributed 2024.1.1 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.1 which is incompatible.\njupyterlab 4.2.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 30.1.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.2 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.3 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask==2024.1.1, but you have dask 2024.5.2 which is incompatible.\nrapids-dask-dependency 24.4.1a0 requires dask-expr==0.4.0, but you have dask-expr 1.1.2 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.3.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kaggle_secrets import UserSecretsClient\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = UserSecretsClient().get_secret(\"OPENAI_API_KEY\")\n",
        "os.environ[\"COHERE_API_KEY\"] = UserSecretsClient().get_secret(\"COHERE_API_KEY\")\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = UserSecretsClient().get_secret(\"ANTHROPIC_API_KEY\")\n",
        "os.environ[\"VOYAGE_API_KEY\"] = UserSecretsClient().get_secret(\"VOYAGE_API_KEY\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:37:57.314135Z",
          "iopub.execute_input": "2024-07-09T00:37:57.314522Z",
          "iopub.status.idle": "2024-07-09T00:37:58.353607Z",
          "shell.execute_reply.started": "2024-07-09T00:37:57.314485Z",
          "shell.execute_reply": "2024-07-09T00:37:58.352665Z"
        },
        "trusted": true,
        "id": "eCiLw2cmlSDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Paper Database"
      ],
      "metadata": {
        "id": "H4x952VJSFeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Processing PDFs into Markdowns\n",
        "We begin by processing PDFs of academic papers scraped from the Arxiv database by converting them into Markdown files. Lin 2024 [1], demonstrated how low accuracy of PDF parsing in RAGs impacts the effectiveness of knowledge-based QA, so we address this problem by equipping the Archimedes' database with an alternative format. Our choice is **Markdown**: a format that allows for consistent formatting for text, formulas and tables.\n",
        "\n",
        "<br/>\n",
        "\n",
        "While there is a number of open source solutions for PDF to Markdown conversion, two stand out as currently leading: Meta's Nougat-OCR [2] and Marker [3]. Both show improved efficiency relative to previously commonly used solution: PyPDF (which was utilized by e.g. ChatGPT).\n",
        "\n",
        "<br/>\n",
        "\n",
        "Nougat-OCR and Matker can be thought of as compliments to each other. Marker allows faster processing, while Nougat-OCR has proven to be more stable in high-accuracy generation. As such, we use a combination for both to develop our database for Archimedes.\n"
      ],
      "metadata": {
        "id": "V4JgXh6HlSDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Loading Markdown Files\n",
        "We trim each text to exclude the bibliography sections in order to remove the content that would be irrelevant for the RAG"
      ],
      "metadata": {
        "id": "PqtpV_BFlSDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Iterator\n",
        "from langchain_core.document_loaders import BaseLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class ArxivMarkdownLoader(BaseLoader):\n",
        "    def __init__(self, file_path: str) -> None:\n",
        "        self.file_path = file_path\n",
        "\n",
        "    def lazy_load(self) -> Iterator[Document]:\n",
        "        try:\n",
        "            with open(self.file_path, 'r', encoding=\"utf-8\") as f:\n",
        "                lines = f.readlines()  # Read all lines to extract the range\n",
        "                abstract_start = None\n",
        "                references_end = None\n",
        "\n",
        "                abstract_str = None\n",
        "                for idx, line in enumerate(lines):\n",
        "                    if \"###### Abstract\" in line:\n",
        "#                         print(\"Found 6# Abstract\")\n",
        "                        abstract_start = idx\n",
        "                        abstract_str = \"###### Abstract\"\n",
        "                    elif \"## References\" in line:\n",
        "                        references_end = idx\n",
        "                        break  # We can stop searching once the end is found\n",
        "\n",
        "                if abstract_start is None or references_end is None or abstract_start >= references_end:\n",
        "                    raise ValueError(\"Could not find the specified section in the file.\")\n",
        "\n",
        "                section_content = abstract_str + \"\".join(lines[abstract_start + 1:references_end])\n",
        "\n",
        "                yield Document(\n",
        "                    page_content=section_content,\n",
        "                    metadata={\"source\": self.file_path}\n",
        "                )\n",
        "        except Exception:\n",
        "            print(f\"Failed to process {self.file_path}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:37:58.354837Z",
          "iopub.execute_input": "2024-07-09T00:37:58.355161Z",
          "iopub.status.idle": "2024-07-09T00:37:58.769241Z",
          "shell.execute_reply.started": "2024-07-09T00:37:58.355128Z",
          "shell.execute_reply": "2024-07-09T00:37:58.768489Z"
        },
        "trusted": true,
        "id": "epuFzxJGlSDi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class ArxivMarkdownDirectoryLoader(BaseLoader):\n",
        "\n",
        "    def __init__(self, directory_path: str, file_extension: str = \".md\") -> None:\n",
        "        self.directory_path = directory_path\n",
        "        self.file_extension = file_extension\n",
        "\n",
        "    def lazy_load(self) -> Iterator[Document]:\n",
        "        file_paths = [\n",
        "            os.path.join(self.directory_path, f)\n",
        "            for f in os.listdir(self.directory_path)\n",
        "            if f.endswith(self.file_extension)\n",
        "        ]\n",
        "\n",
        "        for file_path in file_paths:\n",
        "            loader = ArxivMarkdownLoader(file_path)\n",
        "            # Load documents from the individual file loader\n",
        "            for document in loader.lazy_load():\n",
        "                yield document"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:37:58.771232Z",
          "iopub.execute_input": "2024-07-09T00:37:58.771576Z",
          "iopub.status.idle": "2024-07-09T00:37:58.779222Z",
          "shell.execute_reply.started": "2024-07-09T00:37:58.771550Z",
          "shell.execute_reply": "2024-07-09T00:37:58.778024Z"
        },
        "trusted": true,
        "id": "UMf7ra-olSDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = ArxivMarkdownDirectoryLoader(\"/kaggle/input/arxivlangchain-md2/papers_md\",\n",
        "                                     file_extension = \".mmd\")\n",
        "markdown_documents = loader.load()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:37:58.780352Z",
          "iopub.execute_input": "2024-07-09T00:37:58.780700Z",
          "iopub.status.idle": "2024-07-09T00:37:59.346741Z",
          "shell.execute_reply.started": "2024-07-09T00:37:58.780663Z",
          "shell.execute_reply": "2024-07-09T00:37:59.345740Z"
        },
        "trusted": true,
        "id": "Uca10EPYlSDj",
        "outputId": "3a47a579-971b-468a-aafd-d735dd5b9d20"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Failed to process /kaggle/input/arxivlangchain-md2/papers_md/2403.08822v1.LoRA_SP__Streamlined_Partial_Parameter_Adaptation_for_Resource_Efficient_Fine_Tuning_of_Large_Language_Models.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2206.14077v2.DSME_LoRa__Seamless_Long_Range_Communication_Between_Arbitrary_Nodes_in_the_Constrained_IoT.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2402.13533v1.FinGPT_HPC__Efficient_Pretraining_and_Finetuning_Large_Language_Models_for_Financial_Applications_with_High_Performance_Computing.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2206.09532v1.Hands_on_Wireless_Sensing_with_Wi_Fi__A_Tutorial.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2209.00863v1.Delay_Tolerant_ICN_and_Its_Application_to_LoRa.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2401.13569v1.SPARC_LoRa__A_Scalable__Power_efficient__Affordable__Reliable__and_Cloud_Service_enabled_LoRa_Networking_System_for_Agriculture_Applications.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2302.13724v1.On_the_Use_of_Power_Amplifier_Nonlinearity_Quotient_to_Improve_Radio_Frequency_Fingerprint_Identification_in_Time_Varying_Channels.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2312.16715v1.Adversarial_Attacks_on_LoRa_Device_Identification_and_Rogue_Signal_Detection_with_Deep_Learning.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2308.00634v1.Quasisynchronous_LoRa_for_LEO_Nanosatellite_Communications.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2403.00830v1.MedAide__Leveraging_Large_Language_Models_for_On_Premise_Medical_Assistance_on_Edge_Devices.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2306.16333v1.LoRa_based_Over_the_Air_Computing_for_Sat_IoT.mmd\nFailed to process /kaggle/input/arxivlangchain-md2/papers_md/2402.11896v1.SIBO__A_Simple_Booster_for_Parameter_Efficient_Fine_Tuning.mmd\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(markdown_documents)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:37:59.347967Z",
          "iopub.execute_input": "2024-07-09T00:37:59.348347Z",
          "iopub.status.idle": "2024-07-09T00:37:59.355158Z",
          "shell.execute_reply.started": "2024-07-09T00:37:59.348322Z",
          "shell.execute_reply": "2024-07-09T00:37:59.354101Z"
        },
        "trusted": true,
        "id": "EpprnruilSDj",
        "outputId": "06831907-6a69-420f-f517-4f4bf3a0717e"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "88"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that this only includes a part of our database for the purposes of this demo"
      ],
      "metadata": {
        "id": "mzDfW7vhIsKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Chunking\n",
        "\n",
        "While the division of the papers into coherent subsections is helpful for the RAG, we take our data processing a step further with thew use of chunking. Microsoft Research 2023 [4], showed that chunks of 512 tokens with 25% overlap have strike a balance between capturing enough context and staying within the token limits of many NLP models, ensuring that relevant information is more likely to be retrieved effectively. In the case of Archimedes, a small number of subsections exceed 512 tokens, what also reduces risk of cutting up coherent messages in the described chunking process. As such, we implement it into our RAG."
      ],
      "metadata": {
        "id": "1wiS7ibYlSDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "\n",
        "def flatten(nested_list):\n",
        "        return [item for sublist in nested_list for item in sublist]\n",
        "\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Section\"),\n",
        "    (\"##\", \"Section\"),\n",
        "    (\"######\", \"Section\"),\n",
        "]\n",
        "\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers = False)\n",
        "docs = flatten([markdown_splitter.split_text(d.page_content) for d in markdown_documents])\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:37:59.356560Z",
          "iopub.execute_input": "2024-07-09T00:37:59.356876Z",
          "iopub.status.idle": "2024-07-09T00:37:59.744876Z",
          "shell.execute_reply.started": "2024-07-09T00:37:59.356847Z",
          "shell.execute_reply": "2024-07-09T00:37:59.743911Z"
        },
        "trusted": true,
        "id": "773WTb-5lSDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "CHUNK_SIZE = 512\n",
        "text_splitter = text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            encoding_name=\"cl100k_base\", chunk_size=CHUNK_SIZE, chunk_overlap=int(0.25 * CHUNK_SIZE),\n",
        "            allowed_special={'<|endofprompt|>', '<|endoftext|>',}\n",
        "        )\n",
        "docs = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:48:34.733695Z",
          "iopub.execute_input": "2024-07-09T00:48:34.734331Z",
          "iopub.status.idle": "2024-07-09T00:48:37.579839Z",
          "shell.execute_reply.started": "2024-07-09T00:48:34.734299Z",
          "shell.execute_reply": "2024-07-09T00:48:37.579086Z"
        },
        "trusted": true,
        "id": "rz2Q0ltLlSDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:48:38.449919Z",
          "iopub.execute_input": "2024-07-09T00:48:38.450558Z",
          "iopub.status.idle": "2024-07-09T00:48:38.458016Z",
          "shell.execute_reply.started": "2024-07-09T00:48:38.450516Z",
          "shell.execute_reply": "2024-07-09T00:48:38.457247Z"
        },
        "trusted": true,
        "id": "RrV1J9gmlSDk",
        "outputId": "dc036e29-4f18-4279-f9e7-46d17fc66713"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 16,
          "output_type": "execute_result",
          "data": {
            "text/plain": "2566"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:48:46.850343Z",
          "iopub.execute_input": "2024-07-09T00:48:46.850685Z",
          "iopub.status.idle": "2024-07-09T00:48:46.857064Z",
          "shell.execute_reply.started": "2024-07-09T00:48:46.850660Z",
          "shell.execute_reply": "2024-07-09T00:48:46.855893Z"
        },
        "trusted": true,
        "id": "ut0ct5VUlSDk",
        "outputId": "437c6946-0212-425e-be1b-0abc0f894617"
      },
      "execution_count": null,
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Document(metadata={'Section': 'Abstract'}, page_content=\"###### Abstract\\nAs the key component in multimodal large language models (MLLMs), the ability of the visual encoder greatly affects MLLM's understanding on diverse image content. Although some large-scale pre-trained vision encoders such as vision encoders in CLIP and DINOv2 have brought promising performance, we found that there is still no single vision encoder that can dominate various image content understanding, _e.g._, the CLIP vision encoder leads to outstanding results on general image understanding but poor performance on document or chart content. To alleviate the bias of CLIP vision encoder, we first delve into the inherent behavior of different pre-trained vision encoders and then propose the MoVA, a powerful and novel MLLM, adaptively routing and fusing task-specific vision experts with a coarse-to-fine mechanism. In the coarse-grained stage, we design a context-aware expert routing strategy to dynamically select the most suitable vision experts according to the user instruction, input image, and expertise of vision experts. This benefits from the powerful model function understanding ability of the large language model (LLM) equipped with expert-routing low-rank adaptation (LoRA). In the fine-grained stage, we elaborately conduct the mixture-of-vision-expert adapter (MoV-Adapter) to extract and fuse task-specific knowledge from various experts. This coarse-to-fine paradigm effectively leverages representations from experts based on multimodal context and model expertise, further enhancing the generalization ability. We conduct extensive experiments to evaluate the effectiveness of the proposed approach. Without any bells and whistles, MoVA can achieve significant performance gains over current state-of-the-art methods in a wide range of challenging multimodal benchmarks. Codes and models will be available at [https://github.com/TempleX98/MoVA](https://github.com/TempleX98/MoVA).  \\nKeywords:Multimodal large language modelVision encoder Mixture-of-expert\")"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Vector database\n",
        "\n",
        "The created overlapping chunks need to be processed to a vector database for the RAG architecture. The main decision we faced when developing the vector database was the  choice of embedding model.\n",
        "<br/>\n",
        "\n",
        "Based on Microsoft Research [7], in absence of specialized embedding models for academic papers, we settled on generalist embedding models. We have guided our decision by MTEB benchmark [13], where we have focused both on overall score, but also on applicability. As such, we decided to use the Voyage Large 2 model. It is, in fact, also the model generally recommended by the Anthropic Team [8], and one that has proven to be highly effective for the tasks tackled by Archimedes.\n",
        "<br/>\n",
        "\n",
        "For the implementaion of the database, we chose to use ChromaDB as a test store and Pinecone for the production step.\n"
      ],
      "metadata": {
        "id": "hpC-sb-NlSDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings.fastembed import FastEmbedEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_voyageai import VoyageAIEmbeddings\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def split_list(input_list, chunk_size):\n",
        "    for i in range(0, len(input_list), chunk_size):\n",
        "        yield input_list[i:i + chunk_size]\n",
        "\n",
        "def load_chromadb(embedding_function):\n",
        "    split_docs_chunked = split_list(docs, 1)\n",
        "\n",
        "    vector_db = None\n",
        "    for split_docs_chunk in tqdm(list(split_docs_chunked)):\n",
        "        vector_db = Chroma.from_documents(\n",
        "            documents=split_docs_chunk,\n",
        "            embedding=embedding_function,\n",
        "            persist_directory=DB_PATH,\n",
        "        )\n",
        "\n",
        "    return vector_db\n",
        "\n",
        "\n",
        "DB_PATH = \"./chroma_db\"\n",
        "\n",
        "# embedding_function = FastEmbedEmbeddings()\n",
        "# embedding_function = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "# embedding_function = HuggingFaceEmbeddings(\n",
        "#     model_name=\"Alibaba-NLP/gte-large-en-v1.5\",\n",
        "#     model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "#                'trust_remote_code':True,\n",
        "#                 'torch_dtype':torch.float16\n",
        "#                },\n",
        "#     encode_kwargs={'normalize_embeddings': True}\n",
        "# )\n",
        "\n",
        "embedding_function = VoyageAIEmbeddings(\n",
        "    voyage_api_key=os.environ[\"VOYAGE_API_KEY\"], model=\"voyage-large-2\"\n",
        ")\n",
        "\n",
        "vector_db = load_chromadb(embedding_function)\n",
        "print(\"Finished loading\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T00:48:46.860390Z",
          "iopub.execute_input": "2024-07-09T00:48:46.860794Z",
          "iopub.status.idle": "2024-07-09T01:04:52.292921Z",
          "shell.execute_reply.started": "2024-07-09T00:48:46.860769Z",
          "shell.execute_reply": "2024-07-09T01:04:52.291946Z"
        },
        "trusted": true,
        "id": "iDTXx0PZlSDl",
        "outputId": "074612d2-7934-420b-f37f-2b4ed5d514f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "batch size None\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 2566/2566 [16:05<00:00,  2.66it/s]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Finished loading\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Overview of Archimedes' Search Philosophy\n",
        "\n",
        "Our key principle is to extract as much knowledge from papers as possible without relying on any information that an LLM uses internally (hence the choice of temperature=0 for or output generation). This approach is the most useful to the researchers who would want to gain insights solely based on scholarly research.\n",
        "\n",
        "\n",
        "Please note that in this experiment we have focused on retrieval from embedded documents only. However, in the future, we are planning to use combination of keyword-based search and RAG retrieval (which has been shown as an efficient approach by Microsoft Research [4]),"
      ],
      "metadata": {
        "id": "ujS0DE87lSDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Reranking & Testing\n",
        "\n",
        "To improve extraction of information and further advance the Archimedes RAG solution, we added an additional reranking step [5]. This approach enables the extraction of a number of potentially relevant documents abd then reranking them based on the relevance to the query in order to highlight a subset of top documents (chunks). We used the dedicated Cohere Rerank 3 model, which has proven to be outperforming generalist solutions like GPT-4-turbo, while remaning cost efficient. [6]  \n"
      ],
      "metadata": {
        "id": "AC5fTrOMlSDl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to ask a seemingly simply, but quite ambigious questions -> What is LoRA?\n",
        "\n",
        "In LLMs it is Low-Rank Adaptation used for efficient fine-tuning. Yet this concept also exists in computer networking. Note that our database for the RAG contains papers relevant to LoRA in each of the two (among other) contexts. As such, for a simple RAG this task could present a substential challenge. We begin by checking if our RAG with the rerank accurately selects relevant chunks."
      ],
      "metadata": {
        "id": "J8ArwLkwe2kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "# https://api.python.langchain.com/en/latest/retrievers/langchain.retrievers.document_compressors.cohere_rerank.CohereRerank.html\n",
        "question = \"What is LoRA in LLM?\"\n",
        "\n",
        "reranker = Cohere(temperature=0)\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\", top_n=5)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=vector_db.as_retriever(search_kwargs={\"k\": 20})\n",
        ")\n",
        "\n",
        "retrieved_docs = compression_retriever.invoke(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:04:52.294804Z",
          "iopub.execute_input": "2024-07-09T01:04:52.295581Z",
          "iopub.status.idle": "2024-07-09T01:04:52.910312Z",
          "shell.execute_reply.started": "2024-07-09T01:04:52.295546Z",
          "shell.execute_reply": "2024-07-09T01:04:52.909552Z"
        },
        "trusted": true,
        "id": "P901R6QclSDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\".join([d.page_content for d in retrieved_docs]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:04:52.911336Z",
          "iopub.execute_input": "2024-07-09T01:04:52.911600Z",
          "iopub.status.idle": "2024-07-09T01:04:52.916379Z",
          "shell.execute_reply.started": "2024-07-09T01:04:52.911574Z",
          "shell.execute_reply": "2024-07-09T01:04:52.915552Z"
        },
        "trusted": true,
        "id": "vlPU6ssNlSDl",
        "outputId": "9c71b7e2-1446-4513-e070-09ab94d5f466"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "###### Abstract\nLoRA (Low-Rank Adaptation) [1] has emerged as a preferred method for efficiently adapting Large Language Models (LLMs) with remarkable simplicity and efficacy. This note extends the original LoRA paper by offering new perspectives that were not initially discussed and presents a series of insights for deploying LoRA at scale. Without introducing new experiments, we aim to improve the understanding and application of LoRA.\n\n## 1 Introduction  \nLarge Language Models (LLMs) have achieved significant success across a wide spectrum of Natural Language Processing (NLP) tasks Brown et al. (2020); Yuan et al. (2023); Huang et al. (2023). For practical deployment, fine-tuning these models is essential, as it improves their performance for specific downstream tasks and/or aligns model behaviors with human preferences. Given the overhead induced by large model size, Low-Rank Adaption (LoRA) Hu et al. (2021) comes as a parameter-efficient finetuning mechanism widely adopted to finetune LLMs. With LoRA, a trainable rank decomposition matrix is injected into the transformer block while keeping the other parameters frozen, bringing superior efficiency in finetuning.  \nApart from the efficiency brought by LoRA, another noteworthy aspect lies in LoRA's accessibility, which can be easily shared and seamlessly adopted to downstream tasks 1. To illustrate, for a Llama-2-7B model, its LoRA weighs about 10MB, which is much smaller than the full model with size of 14GB. LoRA enables flexibility in customization. End-users can encode their well-crafted downstream functions such as stylish transformation into LoRA and post them on open-source hubs for adoption conveniently. Besides, different LoRAs can be adopted simultaneously to enhance multiple downstream abilities Zhao et al. (2024); Zhang et al. (2023). Such a share-and-play mode enables much easier model customization.  \nFootnote 1: HuggingFace [https://huggingface.co](https://huggingface.co)\n\n## 1 Introduction  \nWith the rapid advancement of Large Language Models (LLMs) such as GPT3 (Brown et al., 2020), BLOOM (Scao et al., 2022) and LLaMA (Touvron et al., 2023), the successful application of self-supervised pretraining on unlabeled text data has presented unprecedented opportunities for enhancing downstream tasks. However, to fully harness the potential of these LLMs in practical applications, it is also necessary to continuously fine-tuning (Wei et al., 2021; Chung et al., 2022) the LLMs based on the training data of specific tasks to meet the performance requirements of downstream tasks. The substantial number of parameters, often exceeding one billion, makes fine-tuning these LLMs a costly endeavor, demanding a significant investment in computational resources (Figure 0(a)). Therefore, in recent years, Parameter-Efficient Fine-Tuning (PEFT) (Mangrulkar et al., 2022; Zhang et al., 2023) techniques have emerged with the aim of reducing the cost of fine-tuning by freezing certain model weights or introducing smaller trainable modules.  \nIn the continual exploration within this field, a series of methods such as LoRA (Hu et al., 2021), AdaLoRA (Zhang et al., 2023), Adamix (Wang et al., 2022), OLoRA (Dettmers et al., 2023) and LoRAHub (Huang et al., 2023) have emerged, each offering unique perspectives on efficiently fine-tuning Large Language Models for better applicability in downstream tasks. LoRA (Figure 0(b)) introduces the concept of LoRA rank to reduce the number of trainable parameters. AdaLoRA builds upon LoRA's foundation, achieving a search-free approach that greatly simplifies the fine-tuning process. Adamix combines the MoE with Adapters to surpass the performance of LoRA. LoRAHub employs a gradient-free method (Liu et al., 2020) to perform weighted combinations of multiple LoRA weights, thereby better adapting to new downstream tasks.\n\n## 1 Introduction  \nLarge language models like ChatGPT excel in tasks such as writing documents, generating complex code, answering questions, and conducting human-like conversations (Ouyang et al., 2022). With LLMs being increasingly applied in diverse task domains, domain-specific fine-tuning has emerged as a key strategy to enhance their downstream capabilities (Raffel et al., 2020; Chowdhery et al., 2022; Roziere et al., 2023; OpenAI et al., 2023). Nevertheless, these methods are notably expensive, introducing major obstacles in developing large-scale models. Aiming to reduce the cost, Parameter-Efficient Fine-Tuning (PEFT) techniques, like adapter weights (Houlsby et al., 2019), prompt weights (Li and Liang, 2021), and LoRA (Hu et al., 2022) have been proposed to minimize the number of trainable parameters. Among them, LoRA is one of the most widely adopted PEFT techniques, due to its nice property of allowing the adaptor  \nFigure 1: Training loss of LLaMA-2-7B model on Alpaca GPT-4 dataset with Full Parameter Training (FT), LoRA, and LISA.  \nto be merged back to the base model parameters. However, LoRA's superior performance in fine-tuning tasks has yet to reach a point that universally surpasses full parameter fine-tuning in all settings (Ding et al., 2022; Dettmers et al., 2023). In particular, it has been observed that LoRA tends to falter on large-scale datasets during continual pre-training (Lialin et al., 2023), which raises doubts about the effectiveness of LoRA under those circumstances. We attribute this to the much fewer trainable parameters of LoRA compared to the base model, which limits the representation power of LoRA training.\n\nWhile there are many existing research results exploring (privacy-preserved) PEFT in the central setting, the exploration on how to conduct (privacy-preserved) LoRA in the FL setting is still a premature. Directly migrating LoRA methods from the central setting and combining it with FedAvg may not achieve the best performance since other sources of interference in the (privacy-preserving) FL setting, such as noisy gradients and non-iid distribution of data in the cross-silo setting, can play important roles in the optimization process. In real-world LLM applications with privacy concerns, such as federated fine-tuning (Babakiya et al., 2023) or fine-tuning under differential privacy guarantees (Li et al., 2022), the performance of LoRA often suffers deterioration.  \n**Contributions.** In this paper, we identify three discordances in applying LoRA in the privacy-preserved FL setting. The first is presented as a mismatched term brought by the joint local updates and separate global aggregations on the two sets of low-rank matrices of LoRA. The second discordance is that if we employ DP-SGD as the differentially private optimizer for training, the injected noise can be amplified by the locally \"semi-quadratic\" nature of LoRA. Lastly, the choice of one hyper-parameter of LoRA, the scaling factor \\(\\alpha\\), can significantly affect the convergence and performance of the final model, no matter enforcing DP or not.  \nTo resolve these discordances, we propose our solution named **F**ederated **F**reeze **A** **LoRA** (FFA-LoRA). FFA-LoRA freezes the non-zero initialized low-rank matrices and only perform update and aggregation on the zero-initialized matrices, only half as many parameters as LoRA. Beside **FFA-LoRA**'s obvious effect of saving half of the communication and computational cost in FL, we also provide intuitions on why it can alleviate the three aforementioned discordances. We conduct comprehensive experiments to demonstrate the advantages of FFA-LoRA over LoRA in privacy-preserving FL, across different tasks, hyper-parameters and privacy protection levels.  \nWe summarize our contributions as follows:  \n* We explore the conditions in privacy-preserved FL that are discordant with LoRA, and provide explanations on the potential reasons of this performance degradation.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Final response generation\n",
        "\n",
        "After retriving reranked top documents the final stage of advanced RAG pipeline is to form final response. We used Claude 3.5 Sonnet as our LLM, which accoring to the most recent benchmark [14], outperfomed GPT-4o in key capabilities vital to our task: graduate level reasoning and reasoning over text.\n",
        "\n",
        "Having ensured the use of the most capable model, we worked on incorporating the best prompting practices, following those outlined by Schulhoff et al. 2024 [10]:\n",
        "1. We used a role prompt to adjust style of response\n",
        "2. We used a prompt template that allows adaptability\n",
        "3. We added the handling of negative cases\n",
        "4. We added the handling of cases where the model does not have sufficient context data to reduce hallucinations\n",
        "5. We used Chain of Thought prompting to enhance reasoning [9,12]\n",
        "\n",
        "To ensure high quality prompt we also optimized it using the Anthropic Prompt Generator [11].\n",
        "\n",
        "<br/>\n",
        "\n",
        "With this advanced RAG setup, let's see how Archmiedes performs in distinguishing between the two LoRA cases. We are also hoping for an output style that would be desired by a researcher looking for answers while writing a literature review for a paper...\n"
      ],
      "metadata": {
        "id": "RsEkVMH6lSDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "\n",
        "llm = ChatAnthropic(model='claude-3-5-sonnet-20240620', temperature=0)\n",
        "# llm = ChatAnthropic(model='claude-3-sonnet-20240229', temperature=0)\n",
        "# llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are an all-knowing AI research assistant whose role is to answer questions using excerpts from research papers that will be provided to you. The provided papers are chosen very carefully so that they contain the most relevant sources there are available to answer the query. You are speaking to academic researchers.\"),\n",
        "    (\"user\", \"\"\"\n",
        "Keep the following question in mind as you complete this task:\n",
        "```\n",
        "{question}\n",
        "```\n",
        "\n",
        "When working through this question I want you to think step by step. However, do not explicitly spell out the individual steps you take, rather answer the query based on the following guidelines.\n",
        "\n",
        "Here are the excerpts (which form your context to answer the research question) from research papers that may contain relevant information to answer the question:\n",
        "```\n",
        "{context}\n",
        "```\n",
        "\n",
        "Carefully search through the excerpts to try to find information that will help you answer the question.\n",
        "Do not add any outside information or make any assumptions. If the question cannot be fully answered by the excerpts, do not attempt to guess or fill in the gaps - just provide a partial answer using only what is stated in the excerpts. If there is nothing that you find relevant to be able to answer the question, then simply state that you cannot answer based on the information that you have available.\n",
        "Do not be overly verbose.\n",
        "\n",
        "Please also behave as if the excerpts were part of your knowledge - i.e. don't refer to them as 'excerpts' or\n",
        "'provided research papers' that were given to you, but rather as knowledge that you can simply hold internally. Never use phrases like \"in the excerpts provided\", etc. Just imagine that the excerpts you are given are a part of your knowledge base. For example, if you're asked \"What is CAMELoT\", directly provide its definition from the excerpt you're provided (e.g. \"CAMELoT (Consolidated\n",
        "Associative Memory Enhanced Long Transformer) is a memory-augmented architecture designed to handle long input sequences in large language  models (LLMs) without the need for re-training.\"), instead of saying \"according to the documents/excerpts provided, CAMELoT is...\"\n",
        "\n",
        "Always be specific in your references to papers. This means that, for example, if you're asked about methods, you should be naming specific methods, not giving a generic answer. Please also cite the authors and years in APA format.\n",
        "\n",
        "\n",
        "\"\"\")])\n",
        "\n",
        "\n",
        "\n",
        "chain = prompt | llm"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:08:10.369947Z",
          "iopub.execute_input": "2024-07-09T01:08:10.370835Z",
          "iopub.status.idle": "2024-07-09T01:08:10.498734Z",
          "shell.execute_reply.started": "2024-07-09T01:08:10.370792Z",
          "shell.execute_reply": "2024-07-09T01:08:10.497831Z"
        },
        "trusted": true,
        "id": "eNg1giPClSDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is LoRA in LLM?\"\n",
        "context = \"\\n\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
        "\n",
        "response = chain.invoke({\"question\": question,\n",
        "             \"context\": context})\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:08:12.477039Z",
          "iopub.execute_input": "2024-07-09T01:08:12.477439Z",
          "iopub.status.idle": "2024-07-09T01:08:19.255226Z",
          "shell.execute_reply.started": "2024-07-09T01:08:12.477411Z",
          "shell.execute_reply": "2024-07-09T01:08:19.254344Z"
        },
        "trusted": true,
        "id": "AcoeIZHYlSDl",
        "outputId": "1ba9fa6d-46f6-4ff8-e208-179c4ef20803"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "LoRA, which stands for Low-Rank Adaptation, is a parameter-efficient fine-tuning method for Large Language Models (LLMs). Introduced by Hu et al. (2021), LoRA has become a preferred approach for efficiently adapting LLMs due to its simplicity and effectiveness.\n\nThe key idea behind LoRA is to inject a trainable rank decomposition matrix into the transformer block of an LLM while keeping the other parameters frozen. This approach significantly reduces the number of trainable parameters compared to full fine-tuning, making it more computationally efficient.\n\nSome key advantages of LoRA include:\n\n1. Efficiency: LoRA allows for fine-tuning LLMs with much fewer parameters. For example, a LoRA adaptation for a Llama-2-7B model weighs only about 10MB, compared to the full model size of 14GB.\n\n2. Accessibility and shareability: LoRA weights can be easily shared and adopted for downstream tasks, enabling flexible model customization.\n\n3. Simultaneous adoption: Multiple LoRAs can be used together to enhance various downstream abilities (Zhao et al., 2024; Zhang et al., 2023).\n\nHowever, it's worth noting that while LoRA is widely adopted, its performance doesn't universally surpass full parameter fine-tuning in all settings (Ding et al., 2022; Dettmers et al., 2023). Some researchers have observed that LoRA may struggle with large-scale datasets during continual pre-training (Lialin et al., 2023), possibly due to its limited number of trainable parameters compared to the base model.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check now what is LoRA in network communication..."
      ],
      "metadata": {
        "id": "ih-UTGszlSDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "from langchain_community.llms import Cohere\n",
        "\n",
        "question = \"What is LoRA in computer network communication?\"\n",
        "\n",
        "reranker = Cohere(temperature=0)\n",
        "compressor = CohereRerank(model=\"rerank-english-v3.0\", top_n=5)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=vector_db.as_retriever(search_kwargs={\"k\": 20})\n",
        ")\n",
        "\n",
        "retrieved_docs = compression_retriever.invoke(question)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:09:09.846711Z",
          "iopub.execute_input": "2024-07-09T01:09:09.847435Z",
          "iopub.status.idle": "2024-07-09T01:09:10.502612Z",
          "shell.execute_reply.started": "2024-07-09T01:09:09.847402Z",
          "shell.execute_reply": "2024-07-09T01:09:10.501841Z"
        },
        "trusted": true,
        "id": "scjEHBa0lSDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n\".join([d.page_content for d in retrieved_docs]))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:09:12.814796Z",
          "iopub.execute_input": "2024-07-09T01:09:12.815492Z",
          "iopub.status.idle": "2024-07-09T01:09:12.820472Z",
          "shell.execute_reply.started": "2024-07-09T01:09:12.815460Z",
          "shell.execute_reply": "2024-07-09T01:09:12.819628Z"
        },
        "trusted": true,
        "id": "xWe_WBU1lSDm",
        "outputId": "63beff91-cbc4-4eff-80cc-f607df29801a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "## I Introduction  \nLoRa is a popular wireless technology for the Internet of Things (IoT) that achieves long range transmissions (km) at minimal power consumption (mW). The narrowband chirp spread spectrum modulation is robust against interference and doppler effect. LoRa operates in unlicensed subGHz spectrums, which are subject to regional band regulations. LoRaWAN is a cloud-based network architecture for LoRa that organizes all communication between constrained Endnodes (ENs) and user applications. LoRaWAN consists of three components: Application Servers (ASs) provide an interface for business logic implementation; A centralized Network Server (NS) coordinates communication including the PHY configuration, media access, and routes between ASs and ENs; Gateways (GWs) act as a LoRaWAN backbone and mediate packets between ENs and the NS. Three constrains are worth stressing: First, downlink packets are heavily regulated. Regional band restrictions limit the number of downlink packets per GW. Hence, high downlink loads lead to unpredictable and long latencies as well as packet loss. This makes LoRaWAN impractical for many applications. Second, the centralized architecture of LoRaWAN challenges data sharing between users, between ENs, and complicates the development of distributed applications across the Internet. Higher-layer protocols (IP, CoAP) run inefficiently on top of LoRaWAN networks. Third, LoRaWAN requires a permanent infrastructure backhaul. Intermittent GW connectivity prevents data forwarding between the LoRaWAN network and ENs. Since peer to peer communication is impossible between ENs, unreachable GWs prevent communication.\n\n## 2 Background  \n### LoRa Communication  \nLoRa signals utilize chirp spread spectrum (CSS) modulation. CSS involves a sequence of chirps whose frequencies increase linearly with time. Each chirp serves as the fundamental data encoding unit for LoRa signals, with all frequencies within a given bandwidth being used to encode the same data. This maximizes the use of the available bandwidth, allowing LoRa signals to overcome attenuation and interference during transmission. As a result, LoRa technologies can support long-range communication over significant distances. As shown in Figure 1, a LoRa data packet consists of the preamble, start frame delimiter (SFD), and data payload. The preamble contains several uplink chirps used for synchronization between the transmitter (a LoRa node) and receiver (a LoRa gateway). The SFD contains several downlink chirps that indicate the start of the payload. By changing the start frequency of chirps, we can realize data encoding and transmission of the payload.  \nA typical large-scale LoRa network consists of multiple LoRa nodes and LoRa gateways [5]. LoRa nodes collect data from sensors and transmit the data to the gateways. Because LoRa nodes usually transmit small data packets at low frequencies, they mostly remain in sleep mode to conserve power. The LoRa protocol stipulates that the duty cycle for transmitting data from a LoRa node should be kept low, usually not more than 1% [6]. A low duty cycle helps reduce data packet collisions or conflicts among different nodes, ultimately improving the reliability and efficiency of data transmission. In addition, LoRa nodes have a long battery  \nFigure 1: LoRa data packet structurelife, lasting for months or even years, making them an ideal option for remote and long-term deployment in wireless sensor networks.  \n### LaRoa Sensing\n\n## II Background and Related Works  \nThis section reviews the LoRa physical layer and transmission parameters. Then, it examines the related works.  \n### _LoRa overview_  \nThe LoRa protocol is a proprietary technology for a long-range, low-power network. This method employs the Chirp Spread Spectrum (CSS), which is one of the spread spectrum modulations. LoRa wireless devices have become an important part of the wireless IoT infrastructure. It has low efficiency in terms of bits per second, while it can transmit no more than 255 bytes of data per packet, which is sufficient for many IoT applications [9]. The LoRa architecture employs the star-of-star topology, which has three types of devices seen in Figure 1.  \nLoRa makes use of unlicensed sub-gigahertz radio frequency bands like the 433, 868, or 915 MHz industrial, scientific, and medical (ISM) bands, as determined by the region it is deployed in [2]. There are five transmission parameters that can be configured for appropriate sender-receiver communication, impacting communication link quality. The descriptions of these parameters are listed below [2]:  \n* _SF:_ In the spread spectrum LoRa modulation, each bit of the payload message represents multiple chips of information. The symbol rate is the rate at which the spread information is sent. SF is the ratio of the nominal symbol rate to the chip rate, which represents the number of symbols transferred per bit of data. The SF can be selected from 7 to 12. Note that since different spreading factors are orthogonal to one another, the SF must be known in advance on both the transmitting and the receiving sides of the link.\n* _TP:_ The amount of power that an ED should put in to transfer its messages is known as transmission power. The LoRa radio TP may vary in steps of 3 dBm from \\(2\\) to \\(14\\) dBm.\n* _CF:_ The central frequency, measured in Hertz, of a carrier wave that is modulated to transmit signals, is known as the carrier frequency. CF may be configured in the frequencies of 433, 868, and 915 MHz with different step sizes depending on the LoRa chip and the regulation rules.\n\nAdvertising packets are sent on all three advertising channels for any given advertising event. This redundancy makes device discovery more resilient in cases where some of the channels experience interference. This means that scanning for BLE devices on any one of the three advertising channels is as good as a multi-channel scan (sequentially scanning each advertising channel), a fact that we also verified experimentally.\n\n### _LoRa_\n\nLoRa is a proprietary physical layer wireless protocol powering network layer protocols, such as LoRaWAN [34] long-range wide-area networks (LP-WAN) and Sidewalk [35]. LoRa defines all supported modulations and physical layer signaling. On the other hand, LoRaWAN defines a subset of all possible modulations and signal parameters, such as frequency allocations and channel widths. The physical layer of the LoRa protocol was patented by Semtech1, and the specification of LoRaWAN is governed by the LoRa Alliance [34]. Work by security researchers have yielded SDR implementations of the physical LoRa layer [28, 5].\n\nFootnote 1: [https://www.semtech.com/](https://www.semtech.com/)\n\nLoRaWAN uplink channels consists of 64 125 KHz wide channels (centered around \\(f_{c}=903.2+0.2k\\) MHz where \\(k=0...63\\)) and 8 500 KHz wide channels (centered around \\(f_{c}=903+1.6(k-64)\\) MHz where \\(k=64...71\\)). LoRaWAN downlink channels consists of 8 500 KHz wide channels (centered around \\(f_{c}=923.3+0.6k\\) MHz where \\(k=0...7\\)) [36].\n\n**Address Information.** We use the third byte after the sync word to enumerate the LoRa devices under test. Indeed, from the traffic we collected, the value of the third byte consistently changed between four values, corresponding to the IDs of the four YoLink devices under test. Incidentally, this third byte of the payload is part of the 32-bit device address (DevAddr) as specified in LoRaWAN frame format [34]. Furthermore, the first byte of DevAddr is used as a network identifier (NwkID), which is fixed for all devices in the same network. In this context, a network consists of a LoRa gateway and end-devices connected to that gateway.\n\nAdvertising packets are sent on all three advertising channels for any given advertising event. This redundancy makes device discovery more resilient in cases where some of the channels experience interference. This means that scanning for BLE devices on any one of the three advertising channels is as good as a multi-channel scan (sequentially scanning each advertising channel), a fact that we also verified experimentally.\n\n### _LoRa_\n\nLoRa is a proprietary physical layer wireless protocol powering network layer protocols, such as LoRaWAN [34] long-range wide-area networks (LP-WAN) and Sidewalk [35]. LoRa defines all supported modulations and physical layer signaling. On the other hand, LoRaWAN defines a subset of all possible modulations and signal parameters, such as frequency allocations and channel widths. The physical layer of the LoRa protocol was patented by Semtech1, and the specification of LoRaWAN is governed by the LoRa Alliance [34]. Work by security researchers have yielded SDR implementations of the physical LoRa layer [28, 5].\n\nFootnote 1: [https://www.semtech.com/](https://www.semtech.com/)\n\nLoRaWAN uplink channels consists of 64 125 KHz wide channels (centered around \\(f_{c}=903.2+0.2k\\) MHz where \\(k=0...63\\)) and 8 500 KHz wide channels (centered around \\(f_{c}=903+1.6(k-64)\\) MHz where \\(k=64...71\\)). LoRaWAN downlink channels consists of 8 500 KHz wide channels (centered around \\(f_{c}=923.3+0.6k\\) MHz where \\(k=0...7\\)) [36].\n\n**Address Information.** We use the third byte after the sync word to enumerate the LoRa devices under test. Indeed, from the traffic we collected, the value of the third byte consistently changed between four values, corresponding to the IDs of the four YoLink devices under test. Incidentally, this third byte of the payload is part of the 32-bit device address (DevAddr) as specified in LoRaWAN frame format [34]. Furthermore, the first byte of DevAddr is used as a network identifier (NwkID), which is fixed for all devices in the same network. In this context, a network consists of a LoRa gateway and end-devices connected to that gateway.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\\n\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
        "\n",
        "response = chain.invoke({\"question\": question,\n",
        "             \"context\": context})\n",
        "\n",
        "\n",
        "print(response.content)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-07-09T01:09:17.497709Z",
          "iopub.execute_input": "2024-07-09T01:09:17.498060Z",
          "iopub.status.idle": "2024-07-09T01:09:24.968679Z",
          "shell.execute_reply.started": "2024-07-09T01:09:17.498020Z",
          "shell.execute_reply": "2024-07-09T01:09:24.967775Z"
        },
        "trusted": true,
        "id": "FOCoDilolSDm",
        "outputId": "c02e81ef-3b09-440f-98e3-b73ac436ea73"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "LoRa (Long Range) is a wireless communication technology designed for Internet of Things (IoT) applications. It uses chirp spread spectrum (CSS) modulation to achieve long-range transmissions (up to kilometers) while maintaining low power consumption (in the milliwatt range). LoRa operates in unlicensed sub-GHz frequency bands, typically 433 MHz, 868 MHz, or 915 MHz, depending on regional regulations.\n\nKey features of LoRa include:\n\n1. Modulation: LoRa uses CSS modulation, which involves a sequence of chirps with linearly increasing frequencies over time. This modulation technique allows for robust communication against interference and Doppler effects.\n\n2. Data packet structure: A LoRa data packet consists of a preamble (for synchronization), a start frame delimiter (SFD), and a data payload.\n\n3. Network architecture: LoRaWAN is a cloud-based network architecture built on top of LoRa. It includes End Nodes (ENs), Gateways (GWs), a Network Server (NS), and Application Servers (ASs).\n\n4. Transmission parameters: LoRa allows configuration of several parameters, including Spreading Factor (SF), Transmission Power (TP), and Carrier Frequency (CF), which affect communication link quality and range.\n\n5. Low duty cycle: LoRa nodes typically maintain a low duty cycle (usually not more than 1%) to conserve power and reduce packet collisions.\n\n6. Long battery life: Due to its low power consumption, LoRa devices can operate for months or even years on a single battery charge.\n\nIt's important to note that while LoRa provides long-range communication capabilities, it has limitations in terms of data rate and packet size, making it suitable for IoT applications that require infrequent, small data transmissions over long distances.\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that Archimedes effectively responds to the query, presenting an output desired for our target audience of academic researchers.\n",
        "<br/>\n",
        "\n",
        "Due to constrained time, this project has limitations but we wanted to end our work by describing our full vision. We hope that Archimedes could become an AI copilot for scientific research, capable of:\n",
        "1. Scholarly work retrieval (which we showed above)\n",
        "2. Literature review writing\n",
        "3. Research idea generation\n",
        "4. Live research feed\n",
        "\n"
      ],
      "metadata": {
        "id": "vZw_frAIibXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this task is undoubtably ambitious, we continue to be thankful for teams like Anthropic, which create tools that could help bring this vision of more efficient research to reality."
      ],
      "metadata": {
        "id": "T8YWpJc-jTgU"
      }
    }
  ]
}